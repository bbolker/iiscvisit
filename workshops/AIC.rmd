---
title: "AIC and all that"
date: "`r format(Sys.time(), '%H:%M %d %B %Y')`"
author: Ben Bolker
output: rmarkdown::tufte_handout
bibliography: "../iisc.bib"
---

```{r opts,message=FALSE,echo=FALSE,warning=FALSE}
library("knitr")
opts_chunk$set(tidy=FALSE)
```

## Background

* novel approach to *model selection*
* $\underbrace{-2L}_{\textrm{badness of fit}}+{2k}_{\textrm{complexity penalty}}$
* minimize expected difference
* typical method: fit all (?) possible models ("dredge")
* select best model ![](pix/skullcross_tiny.png)
* compute *AIC weights*
* multi-model averaging

## Problem

@mcgill_why_2015, "Why AIC Appeals to Ecologist’s Lowest Instincts": some goals of statistical ecological modeling are

> ... estimation of parameters, testing of hypotheses, exploration of covariation, prediction into new conditions, selecting among choices (e.g. models)  ...

(*could add "quantifying variable importance"*); 

> With AIC you present that classic table of $\Delta$AIC and weights and voila! You’ve sort of implied doing all five statistical goals at once. 

## Advantages

* simple
* non-nested (although cf. @ripley_selecting_2004)
* accounts for model complexity

## AIC goals

* AIC is for *prediction*

## ICs

* distinction among ICs
    * AIC, DIC [@spiegelhalter_bayesian_2002]: prediction
	* BIC: identify true model [@yang_can_2005]
	* AICc: finite-sample correction [@richards_testing_2005; @fletcher_model-averaged_2011]
	* QAIC: correct for overdispersion (important!)
	* CAIC (conditional), DIC: predictions at different hierarchical levels [@ohara_focus_2007]
	
* statistical inconsistency/overfitting argument; consistency vs efficiency [@yang_can_2005] and *tapering effect sizes*: [blog post](http://emdbolker.wikidot.com/blog:aic-vs-bic)
```{r effsize,echo=FALSE,fig.width=10}
par(mfrow=c(1,2),las=1,bty="l",mgp=c(1,0,0),mar=c(2,2,1,1))
eff1 <- seq(1,0,length=20)
eff2 <- rep(c(0.9,0),each=10)
plot(eff1,type="h",axes=FALSE,xlab="effect",ylab="log effect size"); box()
plot(eff2,type="h",axes=FALSE,xlab="effect",ylab="log effect size"); box()
```
* asymptotically equivalent to leave-one-out cross-validation
* still need to respect limits of model complexity [@harrell_regression_2001]
* model-averaged CIs are a good idea, but still represent hypothesis testing

## Model selection

* OK, but why?
* don't take $\Delta \textrm{AIC}>2$ criterion too seriously

## Multimodel averaging

* averaging *predictions* is completely OK
* parameter averaging [@cade_model_2015]
    * must average *predictions*
	* parameters in linear models *may* represent predictions
	* problems with multicollinearity; interaction terms [@schielzeth_simple_2010]; nonlinear models
	* how to average zero values?
* `MuMIn`, `AICcmodavg` packages
* multimodel averaging shrinkage vs. penalized regression (Lasso/elastic net/et al.)

## Weights and variable importance

* are model weights probabilities? of what?
     * prob. of inclusion in true best model?
	 * 'savvy priors'
	 * what would Bayesian probabilities mean?

## Alternatives

* penalized regression not without challenge either, but much faster
* expanded models (with shrinkage?), i.e. don't test point hypotheses

## Good practice

* present $\Delta$AIC, not AIC (`bbmle::AICtab`)
* graphical results: don't just present AIC table
* don't take discrete hypotheses too seriously
* be careful

## References

